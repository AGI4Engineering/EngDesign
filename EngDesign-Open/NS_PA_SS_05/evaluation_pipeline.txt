# evaluation_pipeline.txt

# 1) Compilation
# Compile the LLM-generated DUT (`code.sv`) with the golden testbench (`tb.sv`)
# using Icarus Verilog:
#
#   iverilog -g2012 -o user_tb.vvp code.sv tb.sv

# 2) Simulation
# Run the compiled simulation:
#
#   vvp user_tb.vvp

# 3) Test output format
# The testbench prints “[PASS] Test N” or “[FAIL] Test N” for each subtest:
#   • Test 0 → dout should be zero when sortit=0
#   • Test 1 → sorted output valid flag and ascending order
#   • Test 2 → output stability when sortit stays asserted

# 4) Parsing & scoring
# The Python harness (`evaluate.py`) will:
#   • Capture stdout/stderr from `vvp user_tb.vvp`
#   • Search for “[FAIL] Test 0”, “[FAIL] Test 1”, “[FAIL] Test 2”
#   • Set `test0_pass`, `test1_pass`, `test2_pass` to true if the corresponding “[FAIL]” line is absent
#   • Award points: 30 for Test 0, 35 for Test 1, 35 for Test 2
#   • Assign `confidence = 100`
#   • Emit a JSON summary:
#       {
#         "test0_pass": <true|false>,
#         "test1_pass": <true|false>,
#         "test2_pass": <true|false>,
#         "score": <total_points>,
#         "confidence": 100
#       }
