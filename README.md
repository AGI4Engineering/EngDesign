# EngDesign

**EngDesign** is a benchmark of 101 structured engineeringâ€‘design tasks spanning multiple domains. This repository supports our NeurIPS Datasets & Benchmarks track submission:  
**â€œToward Engineeringâ€¯AGI: Benchmarking the Engineering Design Capabilities of LLMs.â€**

---

## ğŸ“‚ Repository Layout

```text
â”œâ”€â”€ tasks/                 # 101 individual task folders
â”‚   â”œâ”€â”€ <task_id>/        # e.g. XG_01
â”‚   â”‚   â”œâ”€â”€ LLM_prompt.txt      # Prompt presented to the LLM
â”‚   â”‚   â”œâ”€â”€ output_structure.py # Defines the expected JSON/Python output schema via instructor
â”‚   â”‚   â”œâ”€â”€ evaluate.py         # Runs simulations & computes evaluation results
â”‚   â”‚   â”œâ”€â”€ images/             # (Optional) Input images for multimodal tasks
â”‚   â”‚   â””â”€â”€ logs/               # Our evaluation logs
â”‚   â””â”€â”€ â€¦                     
â”œâ”€â”€ iterative_result/      # Logs from iterative design runs with GPTâ€‘4o, o1, o3, o4â€‘mini
â””â”€â”€ evaluation/            # Driver scripts & helpers for running the benchmark
    â””â”€â”€ eval_openai_llm.py
````

---

## ğŸš€ Quickstart

1. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

2. **Configure your OpenAI key**
   Edit the top of `evaluation/eval_openai_llm.py` (or set the `OPENAI_API_KEY` environment variable).

3. **Run the full benchmark**

   ```bash
   python evaluation/eval_openai_llm.py --task_dir tasks
   ```

> **Note:**
>
> * Some tasks rely on external tools (e.g., MATLAB, SPICE), which may not be excuable for all machines.


